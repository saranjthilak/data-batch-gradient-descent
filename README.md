# ğŸ“‰ Batch Gradient Descent from Scratch

This project implements **batch gradient descent** using NumPy to perform linear regression without relying on external ML libraries. Itâ€™s designed to demonstrate a foundational machine learning algorithm in a transparent, hands-on way.

## ğŸš€ Features

- Implementation of linear regression using batch gradient descent
- Custom cost function (Mean Squared Error)
- Manual parameter tuning: learning rate, epochs
- Visualization of the cost function convergence
- Simple, educational code structure

## ğŸ§  Concepts Covered

- Linear regression
- Gradient computation
- Mean squared error (MSE)
- Learning rate tuning
- Overfitting/underfitting intuition

## ğŸ“ Project Structure

```text
data-batch-gradient-descent/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.csv          # Input dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gradient_descent.py      # Core gradient descent implementation
â”‚   â”œâ”€â”€ utils.py                 # Helper functions (e.g., plotting, error calc)
â”‚   â””â”€â”€ main.py                  # Entry point to run the model
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ visualization.ipynb      # Cost function plots & model convergence
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ model_parameters.json    # Saved model weights
â”‚   â””â”€â”€ cost_plot.png            # Training loss curve
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
```bash
git clone https://github.com/saranjthilak/data-batch-gradient-descent.git
```
## ğŸ“Š Example Output
Final MSE loss: 0.01234

Learned parameters: theta_0 = 2.1, theta_1 = 0.89

Cost plot showing convergence

## ğŸ“Œ TODO
Add support for mini-batch and stochastic gradient descent

Add polynomial feature transformation

Explore ridge/lasso regularization

Export trained model to pickle or JSON

ğŸ‘¤ Author
Saran Jaya Thilak
