{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import for you the following dataset of ingredients with their mineral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:43.140811Z",
     "iopub.status.busy": "2025-05-08T11:56:43.140187Z",
     "iopub.status.idle": "2025-05-08T11:56:43.307554Z",
     "shell.execute_reply": "2025-05-08T11:56:43.306009Z",
     "shell.execute_reply.started": "2025-05-08T11:56:43.140790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliment</th>\n",
       "      <th>zinc</th>\n",
       "      <th>phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Durum wheat pre-cooked. whole grain. cooked. u...</td>\n",
       "      <td>0.120907</td>\n",
       "      <td>0.193784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian noodles. plain. cooked. unsalted</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.060329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rice. brown. cooked. unsalted</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.201097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice. cooked. unsalted</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rice. parboiled. cooked. unsalted</td>\n",
       "      <td>0.025189</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliment      zinc  phosphorus\n",
       "0  Durum wheat pre-cooked. whole grain. cooked. u...  0.120907    0.193784\n",
       "1             Asian noodles. plain. cooked. unsalted  0.047859    0.060329\n",
       "2                      Rice. brown. cooked. unsalted  0.156171    0.201097\n",
       "3                             Rice. cooked. unsalted  0.065491    0.045704\n",
       "4                  Rice. parboiled. cooked. unsalted  0.025189    0.045704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/gradient_descent_ingredients_zinc_phosphorous.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 We can visualize a somewhat Linear relationship between the `Phosphorus` and `Zinc`.   \n",
    "\n",
    "Let's use Gradient Descent to find the line of best fit between them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:43.839639Z",
     "iopub.status.busy": "2025-05-08T11:56:43.838531Z",
     "iopub.status.idle": "2025-05-08T11:56:44.092174Z",
     "shell.execute_reply": "2025-05-08T11:56:44.091242Z",
     "shell.execute_reply.started": "2025-05-08T11:56:43.839586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAANytJREFUeJzt3Xl0VPX9//FXEjOTxJABGRIIBtEQNyBAQVJEo9a09GixKN9K0QMUXOoCWuNXARVwJ258UUD9iQv6PQpaUWyBL1ajggvKEeKXUhHZ/IaKCYwle8iE5P7+sBkdMklmhpm5M3eej3NyTnPvnck715T7ms+aYBiGIQAAAItINLsAAACAUCLcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASyHcAAAASznO7AIirbW1Vfv371e3bt2UkJBgdjkAAMAPhmGotrZW2dnZSkzsvG0m7sLN/v37lZOTY3YZAAAgCPv27dOJJ57Y6TVxF266desm6Yebk5GRYXI1AADAHzU1NcrJyfE8xzsTd+GmrSsqIyODcAMAQIzxZ0gJA4oBAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClmBpuNmzYoLFjxyo7O1sJCQlatWpVl6/54IMP9LOf/Ux2u10DBgzQsmXLwl4nACBw1Q1u7T5Qp7LyQ9p9sE7VDW6zS0KcMHVvqfr6eg0ZMkTTpk3TZZdd1uX1e/fu1cUXX6zrrrtOL7/8skpLS3X11VerT58+GjNmTAQqBgD4Y39Vo2au3KoPd7o8xwrznCoZn6/s7qkmVoZ4kGAYhmF2EdIPG2G9+eabGjduXIfXzJw5U2vWrNG2bds8x37/+9+rqqpK69at8+vn1NTUyOFwqLq6mo0zASAMqhvcmr68zCvYtCnMc2rRxGFypNlMqAyxLJDnd0yNudm4caOKioq8jo0ZM0YbN27s8DVNTU2qqanx+gIAhI+rzu0z2EjShp0uueronkJ4xVS4qaioUFZWltexrKws1dTUqLGx0edr5s+fL4fD4fnKycmJRKkAELdqDjd3er62i/PAsYqpcBOM2bNnq7q62vO1b98+s0sCAEvLSEnu9Hy3Ls4Dx8rUAcWB6t27tyorK72OVVZWKiMjQ6mpvgeo2e122e32SJQHAJDkTLepMM+pDR2MuXGmM94G4RVTLTejRo1SaWmp17F33nlHo0aNMqkiAMDRHGk2lYzPV2Ge0+t4YZ5TD43PZzAxws7Ulpu6ujrt2rXL8/3evXv1xRdf6IQTTlC/fv00e/Zsffvtt3rppZckSdddd50WL16s22+/XdOmTdN7772n1157TWvWrDHrVwAA+JDdPVWLJg6Tq86t2sPN6paSLGe6jWCDiDA13Hz++ee64IILPN8XFxdLkqZMmaJly5bpu+++U3l5uef8ySefrDVr1uiWW27R448/rhNPPFHPPvssa9wAQBRypBFmYI6oWecmUljnBgCA2GPZdW4AAAC6QrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWQrgBAACWYureUgAAwDqqG9xy1blVc7hZGanJch5vzv5ihBsAAHDM9lc1aubKrfpwp8tzrDDPqZLx+crunhrRWuiWAgAAx6S6wd0u2EjShp0uzVq5VdUN7ojWQ7gBAADHxFXnbhds2mzY6ZKrjnADAABiSM3h5k7P13ZxPtQINwAA4JhkpCR3er5bF+dDjXADAACOiTPdpsI8p89zhXlOOdMjO2OKcAMAAI6JI82mkvH57QJOYZ5TD43Pj/h0cKaCAwCAY5bdPVWLJg6Tq86t2sPN6paSLGc669wAAIAY5kgzJ8wcjW4pAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKceZXQAAAMeiusEtV51bNYeblZGaLOfxNjnSbGaXBRMRbgAAMWt/VaNmrtyqD3e6PMcK85wqGZ+v7O6pJlYGM9EtBQCISdUN7nbBRpI27HRp1sqtqm5wm1QZzEa4AQDEJFedu12wabNhp0uuOsJNvCLcAABiUs3h5k7P13ZxHtZFuAEAxKSMlOROz3fr4jysi3ADAIhJznSbCvOcPs8V5jnlTGfGVLwyPdwsWbJE/fv3V0pKigoKCrRp06ZOr1+4cKFOO+00paamKicnR7fccosOHz4coWoBANHCkWZTyfj8dgGnMM+ph8bnMx08jpk6FfzVV19VcXGxnn76aRUUFGjhwoUaM2aMduzYoczMzHbXv/LKK5o1a5aef/55nX322fr666/1hz/8QQkJCVqwYIEJvwEAwEzZ3VO1aOIwuercqj3crG4pyXKms85NvEswDMMw64cXFBTorLPO0uLFiyVJra2tysnJ0YwZMzRr1qx210+fPl3bt29XaWmp59itt96qzz77TB999JHPn9HU1KSmpibP9zU1NcrJyVF1dbUyMjJC/BsBAIBwqKmpkcPh8Ov5bVq3lNvt1ubNm1VUVPRjMYmJKioq0saNG32+5uyzz9bmzZs9XVd79uzR2rVrddFFF3X4c+bPny+Hw+H5ysnJCe0vAgAAoopp3VIul0stLS3KysryOp6VlaWvvvrK52uuuOIKuVwunXPOOTIMQ0eOHNF1112nO+64o8OfM3v2bBUXF3u+b2u5AQAA1mT6gOJAfPDBB3rwwQf15JNPasuWLXrjjTe0Zs0a3XfffR2+xm63KyMjw+sLAABYl2ktN06nU0lJSaqsrPQ6XllZqd69e/t8zZw5czRp0iRdffXVkqTBgwervr5e1157re68804lJsZUVgMAAGFgWhqw2WwaPny41+Dg1tZWlZaWatSoUT5f09DQ0C7AJCUlSZJMHBcNAACiiKlTwYuLizVlyhSNGDFCI0eO1MKFC1VfX6+pU6dKkiZPnqy+fftq/vz5kqSxY8dqwYIFGjZsmAoKCrRr1y7NmTNHY8eO9YQcAAAQ30wNNxMmTNDBgwc1d+5cVVRUaOjQoVq3bp1nkHF5eblXS81dd92lhIQE3XXXXfr222/Vq1cvjR07Vg888IBZvwIAAIgypq5zY4ZA5skDAIDoEBPr3AAAAIQD4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFjKcWYXAABANKlucMtV51bN4WZlpCbLebxNjjSb2WUhAIQbAAD+bX9Vo2au3KoPd7o8xwrznCoZn6/s7qkmVoZA0C0FAIB+aLE5OthI0oadLs1auVXVDW6TKkOgCDcAAEhy1bnbBZs2G3a65Koj3MQKwg0AAJJqDjd3er62i/OIHoQbAAAkZaQkd3q+WxfnET0INwAASHKm21SY5/R5rjDPKWc6M6ZiBeEGAABJjjSbSsbntws4hXlOPTQ+PyTTwasb3Np9oE5l5Ye0+2Adg5TDhKngAAD8W3b3VC2aOEyuOrdqDzerW0qynOmhWeeGaeaRQ8sNAAA/4UizKTczXUP79VBuZnrIWmyYZh45hBsAAMKMaeaRRbgBACDMmGYeWYQbAADCjGnmkUW4AQAgzJhmHlmEGwAAwiwS08zxI6aCAwAQAeGcZg5vhBsAACLEkUaYiQS6pQAAgKXQcgMgblU3uOWqc6vmcLMyUpPlPJ5P1YAVEG4AxCWWwgesi24pAHGHpfABayPcAIg7LIUPWBvdUgDiDkvhw4oYQ/Yjwg2AuMNS+LAaxpB5o1sKQNyJtqXwqxvc2n2gTmXlh7T7YB1jfhAQxpC1R8sNgLjTthT+rJVbteGoT7qRXgqfT9w4Vv6MIYu37inCDYC4FA1L4Xf1iXvRxGFx91BC4BhD1h7hBkDcMnspfD5xIxQYQ9YeY24AwCR84kYoRNsYsmhAuAEAk/CJG6HQNobs6IBjxhiyaEG3FACYpO0T9wYfXVPx+okbwYmGMWTRhJYbADAJn7gRSo40m3Iz0zW0Xw/lZqbH9d8PLTcAYCI+cQOhR7gBAJOZPWsLsBrCDQAgIOxhhGhHuAEA+I0VlRELGFAMAPALexghVhBuAAB+8WdFZSAaEG4AAH5hRWXEipCEm5qaGq1atUrbt28P+LVLlixR//79lZKSooKCAm3atKnT66uqqnTjjTeqT58+stvtOvXUU7V27dpgSwcA+IkVlRErggo3l19+uRYvXixJamxs1IgRI3T55ZcrPz9fK1eu9Pt9Xn31VRUXF2vevHnasmWLhgwZojFjxujAgQM+r3e73frlL3+pb775Rq+//rp27NihpUuXqm/fvsH8GgCAALCHEWJFUOFmw4YNOvfccyVJb775pgzDUFVVlZ544gndf//9fr/PggULdM0112jq1Kk688wz9fTTTystLU3PP/+8z+uff/55/etf/9KqVas0evRo9e/fX+edd56GDBnS4c9oampSTU2N1xcAIHCsqIxYkWAYhhHoi1JTU/X1118rJydHkydPVnZ2tkpKSlReXq4zzzxTdXV1Xb6H2+1WWlqaXn/9dY0bN85zfMqUKaqqqtJbb73V7jUXXXSRTjjhBKWlpemtt95Sr169dMUVV2jmzJlKSkry+XPuvvtu3XPPPe2OV1dXKyMjw/9fGgAg6cd1blhRGZFUU1Mjh8Ph1/M7qJabnJwcbdy4UfX19Vq3bp1+9atfSZIOHTqklJQUv97D5XKppaVFWVlZXsezsrJUUVHh8zV79uzR66+/rpaWFq1du1Zz5szRY4891mlr0ezZs1VdXe352rdvn5+/JQDAF/YwQrQLahG/P/3pT7ryyiuVnp6uk046Seeff76kH7qrBg8eHMr6vLS2tiozM1PPPPOMkpKSNHz4cH377bd65JFHNG/ePJ+vsdvtstvtYasJAABEl6DCzQ033KCRI0dq3759+uUvf6nExB8agE455RS/x9w4nU4lJSWpsrLS63hlZaV69+7t8zV9+vRRcnKyVxfUGWecoYqKCrndbtlsfHoAACDeBT0VfMSIEbr00kuVnp7uOXbxxRdr9OjRfr3eZrNp+PDhKi0t9RxrbW1VaWmpRo0a5fM1o0eP1q5du9Ta2uo59vXXX6tPnz4EGwAAICnIlptp06Z1er6j2U5HKy4u1pQpUzRixAiNHDlSCxcuVH19vaZOnSpJmjx5svr27av58+dLkq6//notXrxYN998s2bMmKGdO3fqwQcf1E033RTMrwEAACwoqHBz6NAhr++bm5u1bds2VVVV6Re/+IXf7zNhwgQdPHhQc+fOVUVFhYYOHap169Z5BhmXl5d7urykHwYyv/3227rllluUn5+vvn376uabb9bMmTOD+TUAAIAFBTUV3JfW1lZdf/31ys3N1e233x6KtwyLQKaSAQAQDdqm39ccblZGarKcx8ff9PtAnt8hCzeStGPHDp1//vn67rvvQvWWIUe4AQDEkv1Vje12Yy/Mc6pkfL6yu6eaWFlkhX2dm47s3r1bR44cCeVbAgAQt6ob3O2CjfTDLuyzVm5VdUP7ndirG9zafaBOZeWHtPtgnc9rrC6oMTfFxcVe3xuGoe+++05r1qzRlClTQlIYAADxzlXnbhds2mzY6ZKrzu3VPUUrzw+CCjdlZWVe3ycmJqpXr1567LHHupxJBQAA/FNzuLnT87U/Od9VK8+iicPiZpxOwOHGMAy9+OKL6tWrl1JT4ycFAgAQaRkpyZ2e7/aT84G28lhZwGNuDMPQgAED9M9//jMc9QAAgH9zptva7cLepjDPKWf6j2ElkFYeqws43CQmJiovL0/ff/99OOoBAAD/5kizqWR8fruAU5jn1EPj871aYgJp5bG6oMbclJSU6LbbbtNTTz2lQYMGhbomAADwb9ndU7Vo4jC56tyqPdysbinJcqa3X+emrZVng4+uqaNbeawuqHVuevTooYaGBh05ckQ2m63d2Jt//etfISsw1FjnBgBgVfurGjVr5VavgNPWytMnxmdLBfL8DqrlZuHChcG8DAAQYaxsG1/8beWxuqDCDWvZAED0Y82T+ORIi78wc7Sgwo0ktbS0aNWqVdq+fbskaeDAgbrkkkuUlJQUsuIAAMFhzRPEs6DCza5du3TRRRfp22+/1WmnnSZJmj9/vnJycrRmzRrl5uaGtEgAQGBY8wTxLKi9pW666Sbl5uZq37592rJli7Zs2aLy8nKdfPLJuummm0JdIwAgQKx5gngWVMvN+vXr9emnn+qEE07wHOvZs6dKSko0evTokBUHAAgOa54gngXVcmO321VbW9vueF1dnWw2mjkBwGyBrGwLWE1Q4eY3v/mNrr32Wn322WcyDEOGYejTTz/Vddddp0suuSTUNQIAAhTIyraA1QS1iF9VVZWmTJmiv/71r0pO/qFp88iRI7rkkku0bNkyORyOkBcaKiziByCetK1zE89rnsAawr6IX/fu3fXWW29p586d+uqrryRJZ5xxhgYMGBDM2wEAwoQ1TxCPgl7nRpLy8vKUl5cXqloAxBhWvwUQjYIKNy0tLVq2bJlKS0t14MABtba2ep1/7733QlIcgOjF6rcAolVQ4ebmm2/WsmXLdPHFF2vQoEFKSEgIdV0Aohir3x47Wr2A8Akq3KxYsUKvvfaaLrroolDXAyAGsPrtsaHVCwivoKaC22w2Bg8DcYzVb4PXVatXdYPbpMoA6wgq3Nx66616/PHHFcQscgAWwOq3wfOn1QvAsfG7W+qyyy7z+v69997T//zP/2jgwIGetW7avPHGG6GpDkBUalv9doOPhzSr33aOVi8g/PwON0cvzHfppZeGvBgAsaFt9dtZK7d6BRxWv+0arV5A+Pkdbl544YVw1gEgxmR3T9WiicNY/TZAtHoB4XdMi/gdOHBAO3bskCSddtppyszMDElRAGIDq98GjlYvIPyCCjc1NTW68cYbtWLFCrW0tEiSkpKSNGHCBC1ZsiSq95YCALPR6gWEV1Czpa655hp99tlnWr16taqqqlRVVaXVq1fr888/1x//+MdQ1wgAluNIsyk3M11D+/VQbmY6wQYIoaB2BT/++OP19ttv65xzzvE6/uGHH+rXv/616uvrQ1ZgqLErOAAAsSeQ53dQLTc9e/b02fXkcDjUo0ePYN4SAAAgJIIKN3fddZeKi4tVUVHhOVZRUaHbbrtNc+bMCVlxAAAAgQqqW2rYsGHatWuXmpqa1K9fP0lSeXm57Ha78vLyvK7dsmVLaCoNEbqlACA4bPYJMwXy/A5qttS4ceOCeRkAIAwiETrY7BOxJKiWm1hGyw0Qe2gx6FgkQkd1g1vTl5f53BOrMM+pRROH8d8DYRf2lpt9+/YpISFBJ554oiRp06ZNeuWVV3TmmWfq2muvDeYtAcAnWgw61tUO46EKHf5s9km4QTQJakDxFVdcoffff1/SDwOJi4qKtGnTJt1555269957Q1oggPjV1cO7uiG+d9CO1A7jbPaJWBNUuNm2bZtGjhwpSXrttdc0ePBgffLJJ3r55Ze1bNmyUNYHII5F6uEdqyIVOtjsE7EmqHDT3Nwsu90uSXr33Xd1ySWXSJJOP/10fffdd6GrDkBco8Wgc5EKHW2bffrCZp+IRkGFm4EDB+rpp5/Whx9+qHfeeUe//vWvJUn79+9Xz549Q1oggPhFi0HnIhU62jb7PPpnsdknolVQA4ofeughXXrppXrkkUc0ZcoUDRkyRJL0l7/8xdNdBQDHqu3hvaGDWTrx3mIQyR3G2ewTsSToqeAtLS2qqanx2m7hm2++UVpamjIzM0NWYKgxFRyILfurGjt8ePeJ89lSbdqmyhM6YGWBPL+PaZ2bgwcPaseOHZKk0047Tb169Qr2rSKGcAPEHh7eAMK+zk19fb1mzJihl156Sa2trZKkpKQkTZ48WYsWLVJaWlowbwsAPjnSCDMA/BfUgOLi4mKtX79ef/3rX1VVVaWqqiq99dZbWr9+vW699dZQ1wgAAOC3oLqlnE6nXn/9dZ1//vlex99//31dfvnlOnjwYKjqCzm6pQAAiD1h75ZqaGhQVlZWu+OZmZlqaGgI5i0BoEvsMQXAH0GFm1GjRmnevHl66aWXlJKSIklqbGzUPffco1GjRoW0QACQ2GMKgP+C6pbatm2bxowZo6amJs8aN//7v/+rlJQUvf322xo4cGDICw0VuqWA2MOu1ADC3i01aNAg7dy5Uy+//LK++uorSdLEiRN15ZVXKjWVT1AAQotdqQEEIqhwI0lpaWm65pprQlkLAPjEHlMAAhF0uNm5c6fef/99HThwwLPWTZu5c+cec2EA0IY9pgAEIqhws3TpUl1//fVyOp3q3bu3EhISPOcSEhIINwBCij2mAAQiqAHFJ510km644QbNnDkzHDWFVbgGFDNFFQgv9pgC4lvYBxQfOnRIv/vd74IqzpclS5bokUceUUVFhYYMGaJFixb5tbv4ihUrNHHiRP32t7/VqlWrQlZPoJiiCoQfu1ID8FdQ2y/87ne/09/+9reQFPDqq6+quLhY8+bN05YtWzRkyBCNGTNGBw4c6PR133zzjf7zP/9T5557bkjqCFZ1g7tdsJF+mMExa+VWVTe4TaoMsB5Hmk25meka2q+HcjPTCTYAfPK7W+qJJ57w/O/6+notWLBAF198sQYPHqzkZO/BfDfddJPfBRQUFOiss87S4sWLJUmtra3KycnRjBkzNGvWLJ+vaWlpUWFhoaZNm6YPP/xQVVVVfrfchLpbaveBOl24YH2H50uLz1NuZvox/xwAAOJZWLql/uu//svr+/T0dK1fv17r13s/2BMSEvwON263W5s3b9bs2bM9xxITE1VUVKSNGzd2+Lp7771XmZmZuuqqq/Thhx92+jOamprU1NTk+b6mpsav2vzFFFUAAKKL3+Fm7969Po+3Nfz8dMaUv1wul1paWtrtU5WVleVZHPBoH330kZ577jl98cUXfv2M+fPn65577gm4Nn8xRRWwDiYGANYQ1JgbSXruuec0aNAgpaSkKCUlRYMGDdKzzz4bytraqa2t1aRJk7R06VI5nU6/XjN79mxVV1d7vvbt2xfSmtqmqPrCFFUgduyvatT05WW6cMF6XfrkJ7rwsfWasbxM+6sazS4NQICCmi01d+5cLViwQDNmzPBslLlx40bdcsstKi8v17333uvX+zidTiUlJamystLreGVlpXr37t3u+t27d+ubb77R2LFjPcfaFhA87rjjtGPHDuXm5nq9xm63y263B/T7BcKRZlPJ+PwOp6jyqQ+Ifl1NDGDvKiC2BLXOTa9evfTEE09o4sSJXseXL1+uGTNmyOXyvQeMLwUFBRo5cqQWLVok6Yew0q9fP02fPr3dgOLDhw9r165dXsfuuusu1dbW6vHHH9epp54qm63zf4DCvc4NU1SB2MPEACD6hX2dm+bmZo0YMaLd8eHDh+vIkSMBvVdxcbGmTJmiESNGaOTIkVq4cKHq6+s1depUSdLkyZPVt29fzZ8/39P99VPdu3eXpHbHI82RRpgBYhUTAwBrCSrcTJo0SU899ZQWLFjgdfyZZ57RlVdeGdB7TZgwQQcPHtTcuXNVUVGhoUOHat26dZ5BxuXl5UpMDHpoEAB0iYkBgLUE1S01Y8YMvfTSS8rJydHPf/5zSdJnn32m8vJyTZ482Wvdm6MDkNnC1S0FIHZVN7g1Y3lZh3tXMeYGMF8gz++gws0FF1zg13UJCQl67733An37sCLcAPCFvauA6Bb2cBPLCDcAOsLEACB6hX1AMQBYERMDAGtgpC4AALAUwg0AALAUwg0AALAUxtwACAibSwKIdoQbAH7bX9XYbg+mwjynSsbnK5vp0gCiBN1SAPzS1eaS1Q1ukyoDAG+EGwB+cdW52wWbNht2uuSqI9wAiA6EGwB+YXNJALGCcAPAL2wuCSBWEG4A+MWZblNhntPnucI8p5zpzJgCEB0INwD84kizqWR8fruA07a5JNPBAUQLpoID8Ft291QtmjiMzSUBRDXCDYCAsLkkgGhHtxQAALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUwg0AALAUVihGTKtucMtV51bN4WZlpCbLeTyr5wJAvCPcIGbtr2rUzJVb9eFOl+dYYZ5TJePzld091cTKAABmolsKMam6wd0u2EjShp0uzVq5VdUNbpMqAwCYjZYbxCRXnbtdsGmzYadLrjp3zHdP0eUGAMEh3CAm1Rxu7vR8bRfnox1dbsEjFAIg3CAmZaQkd3q+Wxfno1lXXW73/XaQ/tXg5sHtA6EQgMSYG8QoZ7pNhXlOn+cK85xypsfuA7+rLrddB+t06ZOf6MLH1mvG8jLtr2qMcIWRV93g1u4DdSorP6TdB+t8jqliHBaANoQbxCRHmk0l4/PbBZzCPKceGp8f060ZXXW5NR1p9fzveHhw769q1PTlZbpwwfpOQ50/47AAxAe6pRCzsrunatHEYXLVuVV7uFndUpLlTA++myZaxmp01eVmP877M4lVBlD70lVrzKKJwzy/t9XHYQHwH+EGMc2RFpoAEk1jNdq63Db4aIUYPaCnyvZVtTtu1Qd3ILPirDwOC0Bg6JZC3Iu2sRoddbmNHtBTU0efrOc/2tvuNVZ9cAfSGmPlcVgAAkPLDeJeMGvmhLsL6+gut+Ptx+nz/zukm5aXqcHd4nWtlR/cgbTGtIXCWSu3erV6WWEcFoDAEG4Q9wIdqxGpLqyju9yOtx+n/zmpR1w9uDvrovMV6kI9DgtAbCLcIO4F0joQyADXUIvHB3cwrTGhGocFIHYRbhD3AmkdMHvbh3h8cMdjqANwbBhQjLgXyJo5TDc2hyPNptzMdA3t10O5mekEGwCdouUGkP+tA0w3BoDoR7gB/s2fLp9AB7gCACKPbikgAFbe9gEArIKWGyBADHAFgOhGuAGCEI+zlgAgVtAtBQAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIUVihFW1Q1uuercqjncrIzUZDmPZ2VfAEB4RUXLzZIlS9S/f3+lpKSooKBAmzZt6vDapUuX6txzz1WPHj3Uo0cPFRUVdXo9zLO/qlHTl5fpwgXrdemTn+jCx9ZrxvIy7a9qNLs0AICFmR5uXn31VRUXF2vevHnasmWLhgwZojFjxujAgQM+r//ggw80ceJEvf/++9q4caNycnL0q1/9St9++22EK0dnqhvcmrlyqz7c6fI6vmGnS7NWblV1g9ukyqypusGt3QfqVFZ+SLsP1nF/AcS1BMMwDDMLKCgo0FlnnaXFixdLklpbW5WTk6MZM2Zo1qxZXb6+paVFPXr00OLFizV58uQur6+pqZHD4VB1dbUyMjKOuX6rC7ZbafeBOl24YH2H50uLz1NuZnooS41b+6sa2wXJwjynSsbnK7t7qomVAUDoBPL8NnXMjdvt1ubNmzV79mzPscTERBUVFWnjxo1+vUdDQ4Oam5t1wgkn+Dzf1NSkpqYmz/c1NTXHVnQcOZaHZs3h5k7P13ZxHv7pqoVs0cRhjHECEHdM7ZZyuVxqaWlRVlaW1/GsrCxVVFT49R4zZ85Udna2ioqKfJ6fP3++HA6H5ysnJ+eY644Hx9qtlJGS3On5bl2ch39cde52/43abNjpkquO7ikA8cf0MTfHoqSkRCtWrNCbb76plJQUn9fMnj1b1dXVnq99+/ZFuMrYdKwPTWe6TYV5Tp/nCvOccqZHV2tCrI5ZoYUMANoztVvK6XQqKSlJlZWVXscrKyvVu3fvTl/76KOPqqSkRO+++67y8/M7vM5ut8tut4ek3nhyrA9NR5pNJePzNWvlVm04qlvrofH5UdVVEstjVmghA4D2TA03NptNw4cPV2lpqcaNGyfphwHFpaWlmj59eoeve/jhh/XAAw/o7bff1ogRIyJUbXwJxUMzu3uqFk0cJledW7WHm9UtJVnO9Oha5ybWx6y0tZBt8NHKFo0tZAAQCaZ3SxUXF2vp0qV68cUXtX37dl1//fWqr6/X1KlTJUmTJ0/2GnD80EMPac6cOXr++efVv39/VVRUqKKiQnV1dWb9CpYUqm4lR5pNuZnpGtqvh3Iz06MuKMT6mJW2FrKj/1tFYwsZAESK6SsUT5gwQQcPHtTcuXNVUVGhoUOHat26dZ5BxuXl5UpM/DGDPfXUU3K73fqP//gPr/eZN2+e7r777kiWbmmx1K10LNq639JsSZp2zskaltNdTUdalZKcpC3lh1TfFP1jVmKhhQwAIsn0dW4ijXVuAtO2zo1VH5q7D9Rp7OKP9MTEYXrh4736eNf3nnOjB/TUA+MGq7/zeBMrhNnYQgSIDoE8vwk3iGvVDW6t3Vah1Vv3ewWbNoV5zqgfd4PwieXB5oDVBPL8Nn3MDRBunU3zdqTZ9LN+3X0GGyk2xt0gPNhCBIhdpo+5AcLJn0/eDe6WTt+DtWLikz+DzWnRA6ITLTewLH8/ebNWDHxhgUQgdhFuYFn+TvOOtdWUERmEXiB2EW5gWf5+8matGPhC6AViF2NuYFmO1GRN/8WAdmvXPP/RXjW4W7w+ebNWDI4WL2s9AVZEuIFl2ZISVVZ+SIvf2+U5NnpATz0xcZhe3VTe7pO3I40wA2+EXiA2EW5gSdUNbs1+8+/tpnh/vOt7JUg+P3mzWBt8IfQCsYdwA0vqbDDxR7u+1+HmVq9jLNYGANbBgGJYUiDTeFmsDQCshXADSwpkGm+s7wwOAPBGuIElBTKNNxYWa+tsCwkAgDfG3MCSApnGG+2LtTEeCAACQ7iBZfk7jbetlWeDj64psxdr62o8EDuWA0B7dEvB0hxpNuVmpmtovx7KzUz3GQSieYVixgMBQOBouQEUvYu1xcJ4IACINoQb4N+icbG2aB8PBADRiG4pIIqxeSMABI5wA0SxaB4PBADRim4pBIV9mCInWscDAUC0ItwgYKy7EnnROB4IAKIV3VIICPswAQCiHeEGAWHdFQBAtKNbCj51NKaGdVcAANGOcIN2OhtT40hl3ZVwY7A2ABwbwg28dDWm5pHfDYnafZisgMHaAHDsGHMDL12NqalqcLPuSpgwWBsAQoOWG3jpakzNPw81asRJKZZcd8Xs7iB/BmvH+j0GgEgg3MBLV3sZST88hDvaYTuSQhlGoqE7iMHaABAahBt4cabbdG6e02cLwugBPVW2r0o9jze/9SCUYaSr7qBFE4dFJMixSSYAhAZjbuDFkWbTfb8dpNEDenodHz2gp6aOPlnPf7TX9IdsqMemRMvaPWySCQChQcsN2umRlqzf5Gdr2uiT1XSkVfbjElW2r0o3LS/TiJN6mP6QDfXYlGjpDmrbJHPWyq1es9EYrA0AgSHcoB1Hmk3nndorah+yoQ4j0dQdxCaZAHDsCDfwKZofsqEOI23dQdGydg+bZALAsWHMDTrkSLMpNzNdQ/v1iIrZUW1CPTalrTuItXsAwBoSDMMwzC4ikmpqauRwOFRdXa2MjAyzy+mU2euuRLP9VY0ddpv1CXLqdtv9jraWKgBAYM9vuqWiVDSsuxLNwtltZkhSwjG/DRBSfNgB/Ee4iULRsu6Kr7qi6R/XUI5NIUwimvH3CQSGMTdRKFrWXfmp/VWNmr68TBcuWK9Ln/xEFz62XjOWl2l/VWPEawk19nRCNOPvEwgc4SYKRcu6K22s/o9rNIZJoA1/n0Dg6JYyQVfdO/5OdY5UN5HVN3SMtjAJ/BR/n0DgCDcR5k/fuT/rrkSyD97q/7hG0yJ+wNH4+wQCR7dUBPnbvdPVuiuSItpNZPV/XNnTCdGMv08gcLTcRFAg3TudTXXefaAuot1E0baCb6ixpxOiGX+fQOAINxEUaPdOR1OdI91NFA//uEbzdhMAf59AYAg3ERSq7h0zuoni4R9X9nRCNOPvE/AfY24iKFR952b1wUfrXlMAAPwU4SaCQrVBIxs9AgDQMTbONEGoNmhko0cAQLxg48wo19Z33hZO9rjqlZHqDngRPvrgAQBoj3BjEjbCAwAgPBhzYwKr79UEAICZCDcmYCM8AADCh24pE/hahC/NlqRp55ysYTnd9X29WzpYF7aNMAEAsDLCjQmOXoQvzZakJyYO0wsf79Xi93Z5jjMGBwCAwEVFt9SSJUvUv39/paSkqKCgQJs2ber0+j//+c86/fTTlZKSosGDB2vt2rURqjQ0jl6Eb9o5J+uFj/fq413fe13HGJzIqm5wa/eBOpWVH9Lug3XcdwCIUaaHm1dffVXFxcWaN2+etmzZoiFDhmjMmDE6cOCAz+s/+eQTTZw4UVdddZXKyso0btw4jRs3Ttu2bYtw5cE7ehG+YTnd2wWbNozBiYz9VY2avrxMFy5Yr0uf/EQXPrZeM5aXaX9Vo9mlAQACZPoifgUFBTrrrLO0ePFiSVJra6tycnI0Y8YMzZo1q931EyZMUH19vVavXu059vOf/1xDhw7V008/3eXPi4ZF/Nq0rXPzfb1bl/+/jR1et+qGszW0X48IVhZfqhvcmr68zOcg78I8pxZNHMbYJwAwWSDPb1NbbtxutzZv3qyioiLPscTERBUVFWnjRt8P+40bN3pdL0ljxozp8PqmpibV1NR4fUWLtr2aeh7f+YMzHBth4kfMXgMAazE13LhcLrW0tCgrK8vreFZWlioqKny+pqKiIqDr58+fL4fD4fnKyckJTfEhZNZGmPiBr9lrP1XbxXkAQHQxfcxNuM2ePVvV1dWer3379pldUjtshGmuo2evHY2WMwCILaZOBXc6nUpKSlJlZaXX8crKSvXu3dvna3r37h3Q9Xa7XXa7PTQFh1F291QtmjiMjTBN0NZytqGDMTe0nAFAbDG15cZms2n48OEqLS31HGttbVVpaalGjRrl8zWjRo3yul6S3nnnnQ6vjyVtY3CG9uuh3Mx0gk2E0HIGANZi+iJ+xcXFmjJlikaMGKGRI0dq4cKFqq+v19SpUyVJkydPVt++fTV//nxJ0s0336zzzjtPjz32mC6++GKtWLFCn3/+uZ555hkzfw3EOFrOAMA6TA83EyZM0MGDBzV37lxVVFRo6NChWrdunWfQcHl5uRITf2xgOvvss/XKK6/orrvu0h133KG8vDytWrVKgwYNMutXgEU40ggzAGAFpq9zE2nRtM4NAADwT8yscwMAABBqhBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAphBsAAGAppm+/EGltCzLX1NSYXAkAAPBX23Pbn40V4i7c1NbWSpJycnJMrgQAAASqtrZWDoej02vibm+p1tZW7d+/X926dVNCQkJI37umpkY5OTnat28f+1aFEfc5MrjPkcF9jhzudWSE6z4bhqHa2lplZ2d7bajtS9y13CQmJurEE08M68/IyMjg/zgRwH2ODO5zZHCfI4d7HRnhuM9dtdi0YUAxAACwFMINAACwFMJNCNntds2bN092u93sUiyN+xwZ3OfI4D5HDvc6MqLhPsfdgGIAAGBttNwAAABLIdwAAABLIdwAAABLIdwAAABLIdwEaMmSJerfv79SUlJUUFCgTZs2dXr9n//8Z51++ulKSUnR4MGDtXbt2ghVGtsCuc9Lly7Vueeeqx49eqhHjx4qKirq8r8LfhDo33ObFStWKCEhQePGjQtvgRYR6H2uqqrSjTfeqD59+shut+vUU0/l3w4/BHqfFy5cqNNOO02pqanKycnRLbfcosOHD0eo2ti0YcMGjR07VtnZ2UpISNCqVau6fM0HH3ygn/3sZ7Lb7RowYICWLVsW9jplwG8rVqwwbDab8fzzzxv/+Mc/jGuuucbo3r27UVlZ6fP6jz/+2EhKSjIefvhh48svvzTuuusuIzk52fj73/8e4cpjS6D3+YorrjCWLFlilJWVGdu3bzf+8Ic/GA6Hw/jnP/8Z4cpjS6D3uc3evXuNvn37Gueee67x29/+NjLFxrBA73NTU5MxYsQI46KLLjI++ugjY+/evcYHH3xgfPHFFxGuPLYEep9ffvllw263Gy+//LKxd+9e4+233zb69Olj3HLLLRGuPLasXbvWuPPOO4033njDkGS8+eabnV6/Z88eIy0tzSguLja+/PJLY9GiRUZSUpKxbt26sNZJuAnAyJEjjRtvvNHzfUtLi5GdnW3Mnz/f5/WXX365cfHFF3sdKygoMP74xz+Gtc5YF+h9PtqRI0eMbt26GS+++GK4SrSEYO7zkSNHjLPPPtt49tlnjSlTphBu/BDofX7qqaeMU045xXC73ZEq0RICvc833nij8Ytf/MLrWHFxsTF69Oiw1mkl/oSb22+/3Rg4cKDXsQkTJhhjxowJY2WGQbeUn9xutzZv3qyioiLPscTERBUVFWnjxo0+X7Nx40av6yVpzJgxHV6P4O7z0RoaGtTc3KwTTjghXGXGvGDv87333qvMzExdddVVkSgz5gVzn//yl79o1KhRuvHGG5WVlaVBgwbpwQcfVEtLS6TKjjnB3Oezzz5bmzdv9nRd7dmzR2vXrtVFF10UkZrjhVnPwbjbODNYLpdLLS0tysrK8jqelZWlr776yudrKioqfF5fUVERtjpjXTD3+WgzZ85UdnZ2u/9D4UfB3OePPvpIzz33nL744osIVGgNwdznPXv26L333tOVV16ptWvXateuXbrhhhvU3NysefPmRaLsmBPMfb7iiivkcrl0zjnnyDAMHTlyRNddd53uuOOOSJQcNzp6DtbU1KixsVGpqalh+bm03MBSSkpKtGLFCr355ptKSUkxuxzLqK2t1aRJk7R06VI5nU6zy7G01tZWZWZm6plnntHw4cM1YcIE3XnnnXr66afNLs1SPvjgAz344IN68skntWXLFr3xxhtas2aN7rvvPrNLQwjQcuMnp9OppKQkVVZWeh2vrKxU7969fb6md+/eAV2P4O5zm0cffVQlJSV69913lZ+fH84yY16g93n37t365ptvNHbsWM+x1tZWSdJxxx2nHTt2KDc3N7xFx6Bg/p779Omj5ORkJSUleY6dccYZqqiokNvtls1mC2vNsSiY+zxnzhxNmjRJV199tSRp8ODBqq+v17XXXqs777xTiYl89g+Fjp6DGRkZYWu1kWi58ZvNZtPw4cNVWlrqOdba2qrS0lKNGjXK52tGjRrldb0kvfPOOx1ej+DusyQ9/PDDuu+++7Ru3TqNGDEiEqXGtEDv8+mnn66///3v+uKLLzxfl1xyiS644AJ98cUXysnJiWT5MSOYv+fRo0dr165dnvAoSV9//bX69OlDsOlAMPe5oaGhXYBpC5QGWy6GjGnPwbAOV7aYFStWGHa73Vi2bJnx5ZdfGtdee63RvXt3o6KiwjAMw5g0aZIxa9Ysz/Uff/yxcdxxxxmPPvqosX37dmPevHlMBfdDoPe5pKTEsNlsxuuvv2589913nq/a2lqzfoWYEOh9PhqzpfwT6H0uLy83unXrZkyfPt3YsWOHsXr1aiMzM9O4//77zfoVYkKg93nevHlGt27djOXLlxt79uwx/va3vxm5ubnG5ZdfbtavEBNqa2uNsrIyo6yszJBkLFiwwCgrKzP+7//+zzAMw5g1a5YxadIkz/VtU8Fvu+02Y/v27caSJUuYCh6NFi1aZPTr18+w2WzGyJEjjU8//dRz7rzzzjOmTJnidf1rr71mnHrqqYbNZjMGDhxorFmzJsIVx6ZA7vNJJ51kSGr3NW/evMgXHmMC/Xv+KcKN/wK9z5988olRUFBg2O1245RTTjEeeOAB48iRIxGuOvYEcp+bm5uNu+++28jNzTVSUlKMnJwc44YbbjAOHToU+cJjyPvvv+/z39u2eztlyhTjvPPOa/eaoUOHGjabzTjllFOMF154Iex1JhgG7W8AAMA6GHMDAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADAAAshXADICYtW7ZM3bt3N7sMAFGIFYoBxKTGxkbV1tYqMzPT7FIARBnCDQAAsBS6pQBErW+++UYJCQntvs4///x23VJ33323hg4dqv/+7/9W//795XA49Pvf/161tbWea1pbW/Xwww9rwIABstvt6tevnx544AETfjMA4US4ARC1cnJy9N1333m+ysrK1LNnTxUWFvq8fvfu3Vq1apVWr16t1atXa/369SopKfGcnz17tkpKSjRnzhx9+eWXeuWVV5SVlRWpXwdAhNAtBSAmHD58WOeff7569eqlt956Sy+99JL+9Kc/qaqqStIPLTePPPKIKioq1K1bN0nS7bffrg0bNujTTz9VbW2tevXqpcWLF+vqq6828TcBEG7HmV0AAPhj2rRpqq2t1TvvvKPERN+Nzv379/cEG0nq06ePDhw4IEnavn27mpqadOGFF0akXgDmIdwAiHr333+/3n77bW3atMkrvBwtOTnZ6/uEhAS1trZKklJTU8NaI4DowZgbAFFt5cqVuvfee/Xaa68pNzc36PfJy8tTamqqSktLQ1gdgGhEyw2AqLVt2zZNnjxZM2fO1MCBA1VRUSFJstlsAb9XSkqKZs6cqdtvv102m02jR4/WwYMH9Y9//ENXXXVVqEsHYCJabgBErc8//1wNDQ26//771adPH8/XZZddFtT7zZkzR7feeqvmzp2rM844QxMmTPCMyQFgHcyWAgAAlkLLDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCDQAAsJT/D97g7r0gy4MQAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=data, x='zinc', y='phosphorus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Create the two `np.Array`\n",
    "- `data_X` for zinc\n",
    "- `data_Y` for phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:44.354426Z",
     "iopub.status.busy": "2025-05-08T11:56:44.353981Z",
     "iopub.status.idle": "2025-05-08T11:56:44.360942Z",
     "shell.execute_reply": "2025-05-08T11:56:44.359397Z",
     "shell.execute_reply.started": "2025-05-08T11:56:44.354390Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create numpy arrays\n",
    "data_X = data['zinc'].to_numpy()\n",
    "data_Y = data['phosphorus'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:44.679084Z",
     "iopub.status.busy": "2025-05-08T11:56:44.678788Z",
     "iopub.status.idle": "2025-05-08T11:56:44.685442Z",
     "shell.execute_reply": "2025-05-08T11:56:44.683845Z",
     "shell.execute_reply.started": "2025-05-08T11:56:44.679063Z"
    }
   },
   "outputs": [],
   "source": [
    "assert (data_X.shape == (53,))\n",
    "assert (data_Y.shape == (53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercise, you will define the key functions used to update the parameters during one epoch $\\color {red}{(k)}$ of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "❓ Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:46.591547Z",
     "iopub.status.busy": "2025-05-08T11:56:46.590792Z",
     "iopub.status.idle": "2025-05-08T11:56:46.595746Z",
     "shell.execute_reply": "2025-05-08T11:56:46.594679Z",
     "shell.execute_reply.started": "2025-05-08T11:56:46.591521Z"
    }
   },
   "outputs": [],
   "source": [
    "def h(X,a,b):\n",
    "    return a * X + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "❓ Define the SSR Loss Function for the Hypothesis Function using the equation above. Reuse the function `h` coded above when writing your new function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:48.767144Z",
     "iopub.status.busy": "2025-05-08T11:56:48.766881Z",
     "iopub.status.idle": "2025-05-08T11:56:48.772736Z",
     "shell.execute_reply": "2025-05-08T11:56:48.771627Z",
     "shell.execute_reply.started": "2025-05-08T11:56:48.767125Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(X, Y, a, b):\n",
    "    predictions = h(X, a, b)\n",
    "    errors = predictions - Y\n",
    "    return np.sum(errors ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What would be the total Loss computed on all our ingredients dataset if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:58:43.904727Z",
     "iopub.status.busy": "2025-05-08T11:58:43.904237Z",
     "iopub.status.idle": "2025-05-08T11:58:43.912750Z",
     "shell.execute_reply": "2025-05-08T11:58:43.911667Z",
     "shell.execute_reply.started": "2025-05-08T11:58:43.904676Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.868506986115456"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute and print total loss\n",
    "a = 1\n",
    "b = 1\n",
    "total_loss = loss(data_X, data_Y, a, b)\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2  x_i (y^{(i)} - \\hat{y}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "❓ Define a function to compute the partial derivatives of the Loss Function relative to parameter `a` and `b` at a given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>💡 Hint</summary>\n",
    "Again, you must re-use the Hypothesis Function in your new function to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:03:50.511898Z",
     "iopub.status.busy": "2025-05-08T12:03:50.511042Z",
     "iopub.status.idle": "2025-05-08T12:03:50.516663Z",
     "shell.execute_reply": "2025-05-08T12:03:50.515730Z",
     "shell.execute_reply.started": "2025-05-08T12:03:50.511865Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, a, b):\n",
    "    predictions = h(X, a, b)\n",
    "    errors = predictions - Y\n",
    "    d_a = 2 * np.sum(errors * X)\n",
    "    d_b = 2 * np.sum(errors)\n",
    "    return d_a, d_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:05:49.465179Z",
     "iopub.status.busy": "2025-05-08T12:05:49.464794Z",
     "iopub.status.idle": "2025-05-08T12:05:49.471642Z",
     "shell.execute_reply": "2025-05-08T12:05:49.470464Z",
     "shell.execute_reply.started": "2025-05-08T12:05:49.465157Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.459065809109006, 115.17923733301406)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "partial_derivatives = gradient(data_X, data_Y, a, b)\n",
    "partial_derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Define a function that calculates the step sizes alongside each parameter (`a`,`b`), according to their derivatives (`d_a`, `d_b`) and a `learning_rate` equal to `0.01` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:08:00.771154Z",
     "iopub.status.busy": "2025-05-08T12:08:00.769729Z",
     "iopub.status.idle": "2025-05-08T12:08:00.776049Z",
     "shell.execute_reply": "2025-05-08T12:08:00.775108Z",
     "shell.execute_reply.started": "2025-05-08T12:08:00.771092Z"
    }
   },
   "outputs": [],
   "source": [
    "def step(d_a, d_b, learning_rate=0.01):\n",
    "    step_a = learning_rate * d_a\n",
    "    step_b = learning_rate * d_b\n",
    "    return step_a, step_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What would be the steps (`step_a`, `step_b`) to take for the derivatives computed above for (`a`,`b`) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:12:21.222376Z",
     "iopub.status.busy": "2025-05-08T12:12:21.222090Z",
     "iopub.status.idle": "2025-05-08T12:12:21.229552Z",
     "shell.execute_reply": "2025-05-08T12:12:21.228263Z",
     "shell.execute_reply.started": "2025-05-08T12:12:21.222356Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4845906580910901, 1.1517923733301405)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "steps = step(48.459065809109006, 115.17923733301406, learning_rate=0.01)\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ The steps should be 0.48 for `a` and 1.15 for `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update parameters (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:17:18.540204Z",
     "iopub.status.busy": "2025-05-08T12:17:18.539639Z",
     "iopub.status.idle": "2025-05-08T12:17:18.545850Z",
     "shell.execute_reply": "2025-05-08T12:17:18.544898Z",
     "shell.execute_reply.started": "2025-05-08T12:17:18.540165Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(a, b, step_a, step_b):\n",
    "    a_new = a - step_a \n",
    "    b_new = b - step_b \n",
    "    return a_new , b_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:19:47.094467Z",
     "iopub.status.busy": "2025-05-08T12:19:47.094193Z",
     "iopub.status.idle": "2025-05-08T12:19:47.100780Z",
     "shell.execute_reply": "2025-05-08T12:19:47.099649Z",
     "shell.execute_reply.started": "2025-05-08T12:19:47.094447Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5154093419089099, -0.1517923733301405)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "0.48\n",
    "update_params(a, b, 0.4845906580910901, 1.1517923733301405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the values for `loss`, `a`, and `b` at each epoch to their corresponding lists called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:26:48.832657Z",
     "iopub.status.busy": "2025-05-08T12:26:48.832147Z",
     "iopub.status.idle": "2025-05-08T12:26:48.853941Z",
     "shell.execute_reply": "2025-05-08T12:26:48.849062Z",
     "shell.execute_reply.started": "2025-05-08T12:26:48.832618Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 4.956802678221286, a: 0.5154093419089099, b: -0.1517923733301405\n",
      "Epoch 20, Loss: 1.1061774212625193, a: 0.6908572504746171, b: 0.04072151480231099\n",
      "Epoch 30, Loss: 1.0881159558681284, a: 0.7297825902031633, b: 0.023897654373459715\n",
      "Epoch 40, Loss: 1.0835856008053724, a: 0.7492774973948976, b: 0.015471683875974845\n",
      "Epoch 50, Loss: 1.082449252422271, a: 0.7590411158758447, b: 0.011251711938601302\n",
      "Epoch 60, Loss: 1.0821642223021393, a: 0.7639310209625543, b: 0.009138226832377291\n",
      "Epoch 70, Loss: 1.0820927282334272, a: 0.7663800281419817, b: 0.008079731809064263\n",
      "Epoch 80, Loss: 1.082074795385282, a: 0.7676065624117752, b: 0.007549606609185155\n",
      "Epoch 90, Loss: 1.082070297291267, a: 0.7682208465705213, b: 0.007284104437368382\n",
      "Epoch 100, Loss: 1.0820691690349187, a: 0.7685284980051201, b: 0.007151133201034916\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "# List to store history\n",
    "loss_history = []\n",
    "a_history = []\n",
    "b_history = []\n",
    "\n",
    "# Number of epochs for convergence\n",
    "epochs = 100\n",
    "\n",
    "# Loop through the epochs\n",
    "for epoch in range(epochs):\n",
    "    # Compute gradients for current parameters\n",
    "    d_a, d_b = gradient(data_X, data_Y, a, b)\n",
    "    \n",
    "    # Compute step sizes\n",
    "    step_a, step_b = step(d_a, d_b)\n",
    "    \n",
    "    # Update parameters\n",
    "    a, b = update_params(a, b, step_a, step_b)\n",
    "    \n",
    "    # Calculate loss for current parameters and append to history\n",
    "    current_loss = loss(data_X, data_Y, a, b)\n",
    "    loss_history.append(current_loss)\n",
    "    a_history.append(a)\n",
    "    b_history.append(b)\n",
    "\n",
    "    # Optionally, print the progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+10}, Loss: {current_loss}, a: {a}, b: {b}\")\n",
    "\n",
    "# After 100 epochs, you can access loss_history, a_history, b_history for plotting or analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What are the parameter values at the end of the 100 epochs? Save them to respective variables `a_100` and `b_100` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:25:55.612258Z",
     "iopub.status.busy": "2025-05-08T12:25:55.611978Z",
     "iopub.status.idle": "2025-05-08T12:25:55.617256Z",
     "shell.execute_reply": "2025-05-08T12:25:55.615850Z",
     "shell.execute_reply.started": "2025-05-08T12:25:55.612239Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "a_100=0.768528498005120\n",
    "b_100=0.007151133201034916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:25:56.355031Z",
     "iopub.status.busy": "2025-05-08T12:25:56.354677Z",
     "iopub.status.idle": "2025-05-08T12:25:57.288628Z",
     "shell.execute_reply": "2025-05-08T12:25:57.287685Z",
     "shell.execute_reply.started": "2025-05-08T12:25:56.355008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0 -- /home/saranjthilak92/.pyenv/versions/3.12.9/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/saranjthilak92/code/saranjthilak/05-ML/04-Under-the-hood/data-batch-gradient-descent/tests\n",
      "plugins: typeguard-4.4.2, anyio-4.8.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_descent.py::TestDescent::test_a \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 50%]\u001b[0m\n",
      "test_descent.py::TestDescent::test_b \u001b[32mPASSED\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "💯 You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/descent.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed descent step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Test your code\n",
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('descent',\n",
    "                         a_100=a_100,\n",
    "                         b_100=b_100)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Wrap this iterative approach into a method `gradient_descent()` which returns your `new_a`, `new_b` and `history`, a dictionary containing these lists: \n",
    "- `loss_history`\n",
    "- `a_history`\n",
    "- `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    pass  # YOUR CODE HERE\n",
    "    return a, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize your descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎯 Our goal is to plot our loss function and the gradient descent steps on a 2D surface using matplotlib's `contourf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Start by creating the data we need for the plot\n",
    "- `range_a`: a range of 100 values for `a` equally spaced between -1 and 1\n",
    "- `range_b`: a range of 100 values for `b` equally spaced between -1 and 1 \n",
    "- `Z`: a 2D-array where each element `Z[j,i]` is equal to the value of the loss function at `a` = `range_a[i]` and `b` = `range_b[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Now, plot in one single subplot:\n",
    "- your loss function as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html) with 3 parameters\n",
    "- all historical (a,b) points as a scatterplot with red dots to visualize your gradient descent!\n",
    "\n",
    "Change your learning rate and observe its impact on the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ [optional] What about 3D? Try to plot the same data on a [plot.ly 3D contour plot](https://plotly.com/python/3d-surface-plots/) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "surface = go.Surface(x=range_a, y=range_b, z=Z)\n",
    "scatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\n",
    "fig = go.Figure(data=[surface, scatter])\n",
    "\n",
    "#fig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot the history of the `loss` values as a function of the number of `epochs`. Try with multiple variations of `learning_rate` from 0.001 to 0.01 and make sure to understand the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏁 Congratulation! Please, push your exercise when you are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
