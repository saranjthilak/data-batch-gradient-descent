{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the necessary functions to go through the steps of a single Gradient Descent Epoch. You will then combine the functions and create a loop through the entire Gradient Descent procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import for you the following dataset of ingredients with their mineral content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:43.140811Z",
     "iopub.status.busy": "2025-05-08T11:56:43.140187Z",
     "iopub.status.idle": "2025-05-08T11:56:43.307554Z",
     "shell.execute_reply": "2025-05-08T11:56:43.306009Z",
     "shell.execute_reply.started": "2025-05-08T11:56:43.140790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliment</th>\n",
       "      <th>zinc</th>\n",
       "      <th>phosphorus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Durum wheat pre-cooked. whole grain. cooked. u...</td>\n",
       "      <td>0.120907</td>\n",
       "      <td>0.193784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian noodles. plain. cooked. unsalted</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.060329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rice. brown. cooked. unsalted</td>\n",
       "      <td>0.156171</td>\n",
       "      <td>0.201097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rice. cooked. unsalted</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rice. parboiled. cooked. unsalted</td>\n",
       "      <td>0.025189</td>\n",
       "      <td>0.045704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliment      zinc  phosphorus\n",
       "0  Durum wheat pre-cooked. whole grain. cooked. u...  0.120907    0.193784\n",
       "1             Asian noodles. plain. cooked. unsalted  0.047859    0.060329\n",
       "2                      Rice. brown. cooked. unsalted  0.156171    0.201097\n",
       "3                             Rice. cooked. unsalted  0.065491    0.045704\n",
       "4                  Rice. parboiled. cooked. unsalted  0.025189    0.045704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/gradient_descent_ingredients_zinc_phosphorous.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We can visualize a somewhat Linear relationship between the `Phosphorus` and `Zinc`.   \n",
    "\n",
    "Let's use Gradient Descent to find the line of best fit between them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:43.839639Z",
     "iopub.status.busy": "2025-05-08T11:56:43.838531Z",
     "iopub.status.idle": "2025-05-08T11:56:44.092174Z",
     "shell.execute_reply": "2025-05-08T11:56:44.091242Z",
     "shell.execute_reply.started": "2025-05-08T11:56:43.839586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=data, x='zinc', y='phosphorus');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the two `np.Array`\n",
    "- `data_X` for zinc\n",
    "- `data_Y` for phosphorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:44.354426Z",
     "iopub.status.busy": "2025-05-08T11:56:44.353981Z",
     "iopub.status.idle": "2025-05-08T11:56:44.360942Z",
     "shell.execute_reply": "2025-05-08T11:56:44.359397Z",
     "shell.execute_reply.started": "2025-05-08T11:56:44.354390Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create numpy arrays\n",
    "data_X = data['zinc'].to_numpy()\n",
    "data_Y = data['phosphorus'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:44.679084Z",
     "iopub.status.busy": "2025-05-08T11:56:44.678788Z",
     "iopub.status.idle": "2025-05-08T11:56:44.685442Z",
     "shell.execute_reply": "2025-05-08T11:56:44.683845Z",
     "shell.execute_reply.started": "2025-05-08T11:56:44.679063Z"
    }
   },
   "outputs": [],
   "source": [
    "assert (data_X.shape == (53,))\n",
    "assert (data_Y.shape == (53,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code one Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the exercise, you will define the key functions used to update the parameters during one epoch $\\color {red}{(k)}$ of gradient descent. Recall the formula below\n",
    "\n",
    "$$\n",
    "\\beta_0^{\\color {red}{(k+1)}} = \\beta_0^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_0}(\\beta^{\\color{red}{(k)}})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_1^{\\color {red}{(k+1)}} = \\beta_1^{\\color {red}{(k)}} - \\eta \\frac{\\partial L}{\\partial \\beta_1}(\\beta^{\\color {red}{(k)}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hypothesis Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} =  a x + b\n",
    "$$\n",
    "\n",
    "‚ùì Define the hypothesis function of a Linear Regression. Let `a` be the slope and `b` the intercept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:46.591547Z",
     "iopub.status.busy": "2025-05-08T11:56:46.590792Z",
     "iopub.status.idle": "2025-05-08T11:56:46.595746Z",
     "shell.execute_reply": "2025-05-08T11:56:46.594679Z",
     "shell.execute_reply.started": "2025-05-08T11:56:46.591521Z"
    }
   },
   "outputs": [],
   "source": [
    "def h(X,a,b):\n",
    "    return a * X + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Sum\\ Squares\\ Loss = \\sum_{i=0}^n (y^{(i)} - \\hat{y}^{(i)} )^2\n",
    "$$\n",
    "\n",
    "‚ùì Define the SSR Loss Function for the Hypothesis Function using the equation above. Reuse the function `h` coded above when writing your new function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:56:48.767144Z",
     "iopub.status.busy": "2025-05-08T11:56:48.766881Z",
     "iopub.status.idle": "2025-05-08T11:56:48.772736Z",
     "shell.execute_reply": "2025-05-08T11:56:48.771627Z",
     "shell.execute_reply.started": "2025-05-08T11:56:48.767125Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss(X, Y, a, b):\n",
    "    predictions = h(X, a, b)\n",
    "    errors = predictions - Y\n",
    "    return np.sum(errors ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the total Loss computed on all our ingredients dataset if:\n",
    "- a = 1 \n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T11:58:43.904727Z",
     "iopub.status.busy": "2025-05-08T11:58:43.904237Z",
     "iopub.status.idle": "2025-05-08T11:58:43.912750Z",
     "shell.execute_reply": "2025-05-08T11:58:43.911667Z",
     "shell.execute_reply.started": "2025-05-08T11:58:43.904676Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.868506986115456"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute and print total loss\n",
    "a = 1\n",
    "b = 1\n",
    "total_loss = loss(data_X, data_Y, a, b)\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 63.86. If not, something is wrong with your function. Fix it before moving on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d\\ SSR}{d\\ slope}= \\sum_{i=0}^n -2  x_i (y^{(i)} - \\hat{y}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ SSR}{d\\ intercept}= \\sum_{i=0}^n -2(y^{(i)} - \\hat{y}^{(i)} ) \n",
    "$$\n",
    "\n",
    "‚ùì Define a function to compute the partial derivatives of the Loss Function relative to parameter `a` and `b` at a given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "Again, you must re-use the Hypothesis Function in your new function to compute the predictions at given points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:03:50.511898Z",
     "iopub.status.busy": "2025-05-08T12:03:50.511042Z",
     "iopub.status.idle": "2025-05-08T12:03:50.516663Z",
     "shell.execute_reply": "2025-05-08T12:03:50.515730Z",
     "shell.execute_reply.started": "2025-05-08T12:03:50.511865Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X, Y, a, b):\n",
    "    predictions = h(X, a, b)\n",
    "    errors = predictions - Y\n",
    "    d_a = 2 * np.sum(errors * X)\n",
    "    d_b = 2 * np.sum(errors)\n",
    "    return d_a, d_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using your function, what would be the partial derivatives of each parameter if:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:05:49.465179Z",
     "iopub.status.busy": "2025-05-08T12:05:49.464794Z",
     "iopub.status.idle": "2025-05-08T12:05:49.471642Z",
     "shell.execute_reply": "2025-05-08T12:05:49.470464Z",
     "shell.execute_reply.started": "2025-05-08T12:05:49.465157Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.459065809109006, 115.17923733301406)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "partial_derivatives = gradient(data_X, data_Y, a, b)\n",
    "partial_derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting 48.45 and  115.17. If not, fix your function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Step Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "step\\ size = gradient \\cdot learning\\ rate\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function that calculates the step sizes alongside each parameter (`a`,`b`), according to their derivatives (`d_a`, `d_b`) and a `learning_rate` equal to `0.01` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:08:00.771154Z",
     "iopub.status.busy": "2025-05-08T12:08:00.769729Z",
     "iopub.status.idle": "2025-05-08T12:08:00.776049Z",
     "shell.execute_reply": "2025-05-08T12:08:00.775108Z",
     "shell.execute_reply.started": "2025-05-08T12:08:00.771092Z"
    }
   },
   "outputs": [],
   "source": [
    "def step(d_a, d_b, learning_rate=0.01):\n",
    "    step_a = learning_rate * d_a\n",
    "    step_b = learning_rate * d_b\n",
    "    return step_a, step_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What would be the steps (`step_a`, `step_b`) to take for the derivatives computed above for (`a`,`b`) = (1,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:12:21.222376Z",
     "iopub.status.busy": "2025-05-08T12:12:21.222090Z",
     "iopub.status.idle": "2025-05-08T12:12:21.229552Z",
     "shell.execute_reply": "2025-05-08T12:12:21.228263Z",
     "shell.execute_reply.started": "2025-05-08T12:12:21.222356Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4845906580910901, 1.1517923733301405)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "steps = step(48.459065809109006, 115.17923733301406, learning_rate=0.01)\n",
    "steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The steps should be 0.48 for `a` and 1.15 for `b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Update parameters (a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "updated\\ parameter = old\\ parameter\\ value - step\\ size\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function that computes the updated parameter values from the old parameter values and the step sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:17:18.540204Z",
     "iopub.status.busy": "2025-05-08T12:17:18.539639Z",
     "iopub.status.idle": "2025-05-08T12:17:18.545850Z",
     "shell.execute_reply": "2025-05-08T12:17:18.544898Z",
     "shell.execute_reply.started": "2025-05-08T12:17:18.540165Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_params(a, b, step_a, step_b):\n",
    "    a_new = a - step_a \n",
    "    b_new = b - step_b \n",
    "    return a_new , b_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 One full epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the functions you just created, compute the updated parameters at the end of the first Epoch, had you started with parameters:\n",
    "- a = 1\n",
    "- b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:19:47.094467Z",
     "iopub.status.busy": "2025-05-08T12:19:47.094193Z",
     "iopub.status.idle": "2025-05-08T12:19:47.100780Z",
     "shell.execute_reply": "2025-05-08T12:19:47.099649Z",
     "shell.execute_reply.started": "2025-05-08T12:19:47.094447Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5154093419089099, -0.1517923733301405)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "0.48\n",
    "update_params(a, b, 0.4845906580910901, 1.1517923733301405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è You should be getting the following values:\n",
    "   - updated_a = 0.51\n",
    "   - updated_b = -0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now that you have the necessary functions for a Gradient Descent, loop through epochs until convergence.\n",
    "\n",
    "- Initialize parameters `a = 1` and  `b = 1`\n",
    "- Consider convergence to be **100 epochs**\n",
    "- Don't forget to start each new epoch with the updated parameters\n",
    "- Append the values for `loss`, `a`, and `b` at each epoch to their corresponding lists called `loss_history`, `a_history` and `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:26:48.832657Z",
     "iopub.status.busy": "2025-05-08T12:26:48.832147Z",
     "iopub.status.idle": "2025-05-08T12:26:48.853941Z",
     "shell.execute_reply": "2025-05-08T12:26:48.849062Z",
     "shell.execute_reply.started": "2025-05-08T12:26:48.832618Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 4.956802678221286, a: 0.5154093419089099, b: -0.1517923733301405\n",
      "Epoch 20, Loss: 1.1061774212625193, a: 0.6908572504746171, b: 0.04072151480231099\n",
      "Epoch 30, Loss: 1.0881159558681284, a: 0.7297825902031633, b: 0.023897654373459715\n",
      "Epoch 40, Loss: 1.0835856008053724, a: 0.7492774973948976, b: 0.015471683875974845\n",
      "Epoch 50, Loss: 1.082449252422271, a: 0.7590411158758447, b: 0.011251711938601302\n",
      "Epoch 60, Loss: 1.0821642223021393, a: 0.7639310209625543, b: 0.009138226832377291\n",
      "Epoch 70, Loss: 1.0820927282334272, a: 0.7663800281419817, b: 0.008079731809064263\n",
      "Epoch 80, Loss: 1.082074795385282, a: 0.7676065624117752, b: 0.007549606609185155\n",
      "Epoch 90, Loss: 1.082070297291267, a: 0.7682208465705213, b: 0.007284104437368382\n",
      "Epoch 100, Loss: 1.0820691690349187, a: 0.7685284980051201, b: 0.007151133201034916\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "# List to store history\n",
    "loss_history = []\n",
    "a_history = []\n",
    "b_history = []\n",
    "\n",
    "# Number of epochs for convergence\n",
    "epochs = 100\n",
    "\n",
    "# Loop through the epochs\n",
    "for epoch in range(epochs):\n",
    "    # Compute gradients for current parameters\n",
    "    d_a, d_b = gradient(data_X, data_Y, a, b)\n",
    "    \n",
    "    # Compute step sizes\n",
    "    step_a, step_b = step(d_a, d_b)\n",
    "    \n",
    "    # Update parameters\n",
    "    a, b = update_params(a, b, step_a, step_b)\n",
    "    \n",
    "    # Calculate loss for current parameters and append to history\n",
    "    current_loss = loss(data_X, data_Y, a, b)\n",
    "    loss_history.append(current_loss)\n",
    "    a_history.append(a)\n",
    "    b_history.append(b)\n",
    "\n",
    "    # Optionally, print the progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+10}, Loss: {current_loss}, a: {a}, b: {b}\")\n",
    "\n",
    "# After 100 epochs, you can access loss_history, a_history, b_history for plotting or analysis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What are the parameter values at the end of the 100 epochs? Save them to respective variables `a_100` and `b_100` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:25:55.612258Z",
     "iopub.status.busy": "2025-05-08T12:25:55.611978Z",
     "iopub.status.idle": "2025-05-08T12:25:55.617256Z",
     "shell.execute_reply": "2025-05-08T12:25:55.615850Z",
     "shell.execute_reply.started": "2025-05-08T12:25:55.612239Z"
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "a_100=0.768528498005120\n",
    "b_100=0.007151133201034916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:25:56.355031Z",
     "iopub.status.busy": "2025-05-08T12:25:56.354677Z",
     "iopub.status.idle": "2025-05-08T12:25:57.288628Z",
     "shell.execute_reply": "2025-05-08T12:25:57.287685Z",
     "shell.execute_reply.started": "2025-05-08T12:25:56.355008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.9, pytest-8.3.4, pluggy-1.5.0 -- /home/saranjthilak92/.pyenv/versions/3.12.9/envs/lewagon/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/saranjthilak92/code/saranjthilak/05-ML/04-Under-the-hood/data-batch-gradient-descent/tests\n",
      "plugins: typeguard-4.4.2, anyio-4.8.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_descent.py::TestDescent::test_a \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 50%]\u001b[0m\n",
      "test_descent.py::TestDescent::test_b \u001b[32mPASSED\u001b[0m\u001b[32m                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/descent.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed descent step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üß™ Test your code\n",
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('descent',\n",
    "                         a_100=a_100,\n",
    "                         b_100=b_100)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Wrap this iterative approach into a method `gradient_descent()` which returns your `new_a`, `new_b` and `history`, a dictionary containing these lists: \n",
    "- `loss_history`\n",
    "- `a_history`\n",
    "- `b_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, a_init=1, b_init=1, learning_rate=0.001, n_epochs=100):\n",
    "    pass  # YOUR CODE HERE\n",
    "    return a, b, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the line of best fit through Zinc and Phosphorus using the parameters of your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize your descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Our goal is to plot our loss function and the gradient descent steps on a 2D surface using matplotlib's `contourf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Start by creating the data we need for the plot\n",
    "- `range_a`: a range of 100 values for `a` equally spaced between -1 and 1\n",
    "- `range_b`: a range of 100 values for `b` equally spaced between -1 and 1 \n",
    "- `Z`: a 2D-array where each element `Z[j,i]` is equal to the value of the loss function at `a` = `range_a[i]` and `b` = `range_b[j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, plot in one single subplot:\n",
    "- your loss function as a 2D-surface using matplotlib [contourf](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html) with 3 parameters\n",
    "- all historical (a,b) points as a scatterplot with red dots to visualize your gradient descent!\n",
    "\n",
    "Change your learning rate and observe its impact on the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì [optional] What about 3D? Try to plot the same data on a [plot.ly 3D contour plot](https://plotly.com/python/3d-surface-plots/) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "surface = go.Surface(x=range_a, y=range_b, z=Z)\n",
    "scatter = go.Scatter3d(x=history['a'], y=history['b'], z=history['loss'], mode='markers')\n",
    "fig = go.Figure(data=[surface, scatter])\n",
    "\n",
    "#fig.update_layout(title='Loss Function', autosize=False, width=500, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the history of the `loss` values as a function of the number of `epochs`. Try with multiple variations of `learning_rate` from 0.001 to 0.01 and make sure to understand the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. With Sklearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using Sklearn, train a Linear Regression model on the same data. Compare its parameters to the ones computed by your Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They should be almost identical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†üèÅ Congratulation! Please, push your exercise when you are done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
